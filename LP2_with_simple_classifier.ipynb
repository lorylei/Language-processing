{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4736b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: gensim in /mistgpu/site-packages (4.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.8.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /mistgpu/site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.3)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from xml.dom import minidom\n",
    "! pip install gensim\n",
    "import gensim.downloader as api\n",
    "model = api.load('glove-twitter-100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15707a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydoc = minidom.parse('./pan22-author-profiling-training-2022-03-29/en/11790334350e58e6c92fdeb1d10bc161.xml')\n",
    "\n",
    "print(mydoc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9d4176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse('./pan22-author-profiling-training-2022-03-29/en/11790334350e58e6c92fdeb1d10bc161.xml')\n",
    "root = tree.getroot()\n",
    "\n",
    "print(root[0][10].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbbbde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#number of tweets in a file\n",
    "N = len(root[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a166a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(0,N):\n",
    "    print(root[0][n].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fb9d0b-0f03-4b40-bdef-c564d982a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "data_tok = []\n",
    "for i in range(0,N):  \n",
    "    j=str(root[0][i].text)\n",
    "    data_tok.append([k.lower() for k in tokenizer.tokenize(j)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07067ca7-c072-441a-ba26-270f2472d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_phrase_embedding(phrase):\n",
    "\n",
    "    \n",
    "    vector = np.zeros([model.vector_size], dtype='float32')\n",
    "    \n",
    "    phrase = phrase.lower()\n",
    "    phrase = tokenizer.tokenize(phrase)\n",
    "    \n",
    "    phrase_vectors = []\n",
    "    \n",
    "    for i in phrase:\n",
    "        if i in model.key_to_index.keys():\n",
    "            phrase_vectors.append(model.get_vector(i))\n",
    "    \n",
    "    phrase_vectors = np.array(phrase_vectors)\n",
    "    \n",
    "    if len(phrase_vectors) == 0:\n",
    "        return vector\n",
    "    \n",
    "    phrase_vectors = np.mean(phrase_vectors, axis=0)\n",
    "    \n",
    "    return phrase_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ff9a5-a106-4900-bce5-4713646e2159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7957e6a4-913c-402a-8e04-d64951e037f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_vector(phrase):\n",
    "    data = []\n",
    "    N = len(phrase[0])\n",
    "    for n in range(0,N):\n",
    "        vector = get_phrase_embedding(str(phrase[0][n].text))\n",
    "        data.append(vector)\n",
    "    data1 = np.array(data).reshape(20000,)#.reshape(20000,)\n",
    "   # print(data1.shape)\n",
    "    return data1\n",
    "    \n",
    "vector = get_person_vector(root)\n",
    "#print(len(vector[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32235513-b240-459c-a727-ed1bf9f86c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = \"./pan22-author-profiling-training-2022-03-29/en\" \n",
    "test_path = \"./pan22-author-profiling-training-2022-03-29/en\"\n",
    "files= os.listdir(path) \n",
    "test_files = os.listdir(test_path) \n",
    "data = []\n",
    "GT = \"./pan22-author-profiling-training-2022-03-29/en/truth.txt\"\n",
    "true_values = {}\n",
    "f=open(GT)\n",
    "for line in f:\n",
    "    linev = line.strip().split(\":::\")\n",
    "    true_values[linev[0]] = linev[1]\n",
    "f.close()\n",
    "\n",
    "for file in files: \n",
    "    if str(os.path.basename(file)) == 'truth.txt':\n",
    "        continue\n",
    "    if not os.path.isdir(file):\n",
    "        tree = ET.parse(path+\"/\"+file)\n",
    "        root = tree.getroot()\n",
    "        vector = get_person_vector(root)\n",
    "        \n",
    "        if true_values[str(os.path.basename(file)).strip('.xml')]=='I':\n",
    "            data.append([vector,0])\n",
    "        else:\n",
    "            data.append([vector,1])\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e5f2d2b-fe35-46bf-8479-37a535480775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: neurokit2 in /mistgpu/site-packages (0.2.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from neurokit2) (3.5.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from neurokit2) (1.4.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from neurokit2) (1.22.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from neurokit2) (1.8.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from neurokit2) (1.1.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->neurokit2) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=1.0.0->neurokit2) (3.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (4.33.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->neurokit2) (9.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->neurokit2) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib->neurokit2) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: seaborn in /mistgpu/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.9/dist-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.9/dist-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.9/dist-packages (from seaborn) (1.8.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.9/dist-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "\u001b[33mWARNING: There was an error checking the latest version of pip.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import svm\n",
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "! pip install neurokit2\n",
    "import neurokit2 as nk\n",
    "! pip install seaborn\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f94aff91-0bb8-4597-945a-b964363f81a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 2) (60, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.DataFrame(data) \n",
    "train, test = train_test_split(df, test_size=60)\n",
    "X_train=list(train[0].values)\n",
    "X_test = list(test[0].values)\n",
    "y_train=train[1].values\n",
    "y_test = test[1].values\n",
    "print(train.shape,test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a01af0d-4118-49c2-866e-982562a369a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clf(clf):\n",
    "    score = []\n",
    "    runtime = []\n",
    "    clf.fit(X_train, y_train)\n",
    "    start = time.time()\n",
    "    score.append(clf.score(X_test, y_test))\n",
    "    runtime.append(time.time() - start)\n",
    "\n",
    "    return score, runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f398b82c-e7c1-45a7-a356-eb8b3aa28cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\",\n",
    "         \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\",\n",
    "         \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5,\n",
    "                           n_estimators=10,\n",
    "                           max_features=1),\n",
    "    MLPClassifier(alpha=1,\n",
    "                  max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB()]\n",
    "\n",
    "for name, classifier in zip(names, classifiers):\n",
    "    clf = make_pipeline(MinMaxScaler(), classifier)\n",
    "    score, runtime = run_clf(clf)\n",
    "    results.append([name,\n",
    "                    np.mean(score),\n",
    "                    np.mean(runtime)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa40fde-15cf-4d25-ba28-2ad2450be8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, columns=['name',\n",
    "                                            'mean_score',\n",
    "                                            'mean_runtime'])\n",
    "results_df.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf99940-9baf-4d66-919d-061b1be3b3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e5ac354-1f58-49a3-bf4b-fc2423b09e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 1 0 0 1 1 1 1 0 0 1 1 0 0 0 1 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 0 0 1 1 0]\n",
      "0.8833333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data) \n",
    "train, test = train_test_split(df, test_size=60)\n",
    "model_svm = SVC(kernel='linear', probability=True)\n",
    "#train1=train[0].values.reshape(360,-1)\n",
    "model_svm.fit(list(train[0].values),train[1].values)\n",
    "\n",
    "pre1 = model_svm.predict(list(test[0].values))\n",
    "print(pre1)\n",
    "print(sum(pre1==test[1].values)/60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a71566b0-6cd3-4f2b-a1cf-b19d93316d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=20000, hidden_layer_size=100, output_size=1):\n",
    "       \n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_x):\n",
    "        input_x = input_x.view(len(input_x), 1, -1)\n",
    "        hidden_cell = (torch.zeros(1, 1, self.hidden_layer_size),  # shape: (n_layers, batch, hidden_size)\n",
    "                       torch.zeros(1, 1, self.hidden_layer_size))\n",
    "        lstm_out, (h_n, h_c) = self.lstm(input_x, hidden_cell)\n",
    "        linear_out = self.linear(lstm_out.view(len(input_x), -1))  # =self.linear(lstm_out[:, -1, :])\n",
    "        predictions = self.sigmoid(linear_out)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "84fb972b-2e4f-42d2-a460-f304a37f91b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(420, 200, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [30]\u001b[0m, in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:  \n\u001b[1;32m     37\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m lstm(seq)\u001b[38;5;241m.\u001b[39msqueeze()  \n\u001b[0;32m---> 38\u001b[0m     single_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(y_pred\u001b[38;5;241m==\u001b[39mlabels)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_pred)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py:612\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 612\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:3056\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3054\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m   3055\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[0;32m-> 3056\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3057\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3058\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m   3059\u001b[0m     )\n\u001b[1;32m   3061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3062\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[0;31mValueError\u001b[0m: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "a = np.ones((420,200,100))\n",
    "b = df[0].values\n",
    "for i in range(420):\n",
    "    a[i]=b[i].reshape(200,100)\n",
    "print(a.shape)\n",
    "    \n",
    "x, y = torch.from_numpy(a.astype(float)).to(torch.float32), torch.from_numpy(np.array(df[1].values)).to(torch.float32)\n",
    "\n",
    "train_loader = Data.DataLoader(\n",
    "        dataset=Data.TensorDataset(x, y), \n",
    "        batch_size=1,  \n",
    "        shuffle=True,  \n",
    "        num_workers=2, \n",
    "    )\n",
    "lstm = LSTM()  \n",
    "loss_function = nn.BCELoss()  # loss\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=0.001)  \n",
    "epochs = 10\n",
    "    \n",
    "lstm.train()\n",
    "for i in range(epochs):\n",
    "    for seq, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = lstm(seq).squeeze()  \n",
    "        labels = labels.squeeze()\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "           \n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "lstm.eval()\n",
    "\n",
    "for seq, labels in train_loader:  \n",
    "    y_pred = lstm(seq).squeeze()  \n",
    "    labels = labels.squeeze()\n",
    "    single_loss = loss_function(y_pred, labels)\n",
    "    acc = sum(y_pred==labels)/len(y_pred)\n",
    "#    print(\"EVAL Step:\", i, \" accuracy: \", acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c46de263-4449-493c-9864-965680a37e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0]\n",
      "0.85\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((60,200,100))\n",
    "b = test[0].values\n",
    "for i in range(60):\n",
    "    a[i]=b[i].reshape(200,100)\n",
    "x, y = torch.from_numpy(a.astype(float)).to(torch.float32), torch.from_numpy(np.array(train[1].values)).to(torch.float32)\n",
    "y_test_pred = lstm(x).squeeze()  \n",
    "result = []\n",
    "for i in y_test_pred:\n",
    "    if i >0.5:\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(0)\n",
    "print(result)\n",
    "\n",
    "acc = sum(result==test[1].values)/60\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4abd73-480e-483a-8916-ed371bd86034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
